什么是Nacos：一个更易于构建云原生应用的动态服务发现，配置管理和服务管理平台。
就是注册中心+配置中心的组合。
Nacos=Eureka+Config+ Bus ,用过后面三个的，都会知道有很多bug

下载地址
https://github.com/alibaba/nacos/tags

解压之后，一定搭建好nacos的数据库环境配置
conf\nacos-mysql.sql 弄到数据库中

运行bin下面的startup.cmd   单机启动  默认是集群
startup.cmd -m standalone

之后创建服务提供者cloudalibaba-provider-payment9003 | cloudalibaba-provider-payment9002
nacos会自动提供负载均衡

然后编写服务调用者
注意一定要添加配置类 否则无法使用服务的负载均衡

几种注册中心的比较
名称                      CAP模型               控制台管理           社区活跃度
Eureka                      AP                  支持              低 （2.x版本闭源）
Zookeeper                   CP                  不               中
Consul                      CP                  支持              高
Nacos                       AP                  支持              高

目前NacosAP和CP 2种都是支持的。
CAP理论核心：一个分布式系统不可能同时很好的满足 一致性，可用性，分区容错 这三个需求。
将Nosql分成了 CA  CP  AP
CA,单点集群，满足一致性，可用性， 可扩展性不太强大
CP 一致性，分区容忍，性能不是特别高
AP 可用性，分区容忍  通常可能对一致性要求低一些。

C是所有节点在同一时间看到的数据是一致的，而A的定义是所有的请求都会收到响应

何时选择用何种模式？
一般来说
如果不需要存储服务级别的信息且服务实例是通过nacos-client
注册，并能够保持心跳上报，那么就可可以选择AP模式，当前主流的服务如SpringCloud，Dubbo，
都适用于AP模式，AP模式为了服务的可能性而减弱了一致性，因此AP模式下只支持注册临时实例。


如果需要在服务级别编辑或存储配置信息，那么CP是必须，K8S服务和DNS服务则适用于CP模式。
CP模式下则支持注册持久化实例，此时则是以Raft协议为集群运行模式，该模式下注册实例之前必须
先注册服务，如果服务不存在，则会返回错误。

这样切换
curl -X PUT '$NACOS_SERVER:8848/nacos/v1/ns/operator/switches?entry=serverMode&value=CP'

Nacos作为配置中心
新建cloudalibaba-config-nacos-client3377
@RefreshScope 通过Spring Cloud原生注解RefreshScope实现配置自动更新


在Nacos SpringCloud中  dataId完整格式如下
${prefix}-${spring.profile.active}.${file-extension}

prefix默认为spring.application.name的值,也可以通过配置项spring.cloud.nacos.config.prefix
来配置。

spring.profile.active即为当前环境对应的profile,详情可以参考SpringBoot文档。
注意：当spring.profile.active为空时,对应的连接符 - 也将不存在，dataId的拼接格式
变成${prefix}.${file-extension}



file-extension为配置内容的数据格式，可以通过配置项 spring.cloud.nacos.config.file-extension
来配置。目前只支持properties和yaml类型。

最好先成全名 不要省略${prefix}
创建nacos-config-client-dev.yaml文  内容为  info后面的可以为字符串 也可以不加字符串
config:
    info: config in for dev ,from nacos config center.verson=1

之后就修改config配置文件，直接调用会直接刷新。


Namespace和Group+DataID 三者的关系
类似于java的包名和类名
最外层的namespace是可以用于区分部署环境的。Group和DataID逻辑上区分两个目标对象
默认情况
Namespace=public            Group=DEFAULT_GROUP         默认Cluster是DEFAULT

Nacos默认的命令空间是public，Namespace主要用来实现隔离
比方说我们现在有三个环境，开发，测试，生产环境，我们就可以创建三个Namespace，
不同的Namespace之间是隔离的。
Group默认是DEFAULT_GROUP，Group可以把不同的微服务划分到同一个分组里面去。

Service就是微服务；一个service可以包含多个Cluster（集群）
Nacos默认Cluster是DEFAULT，Cluster是对指定微服务的一个虚拟划分。

比方说为了容灾，将Service微服务分别部署在杭州机房和广州机房，
这时就可以给杭州机房的service微服务起一个集群名称（HZ）
给广州的机房Service微服务起一个集群名称（GZ），还可以尽量让同一个机房的微服务互相调用，
以提升性能。
最后是Instance就是微服务的实例。


搭建nacos集群
nginx下载地址
http://nginx.org/en/download.html
下载稳定版1.18 或者最新版1.19

nginx集群+3台nacos机器+数据库集群（或者主从备份）
先安装依赖包
yum -y install pcre-devel

yum -y install openssl-devel

yum -y install gcc

yum -y install lrzsz

yum -y install openssh-clients

或者输入
yum -y install pcre-devel openssl-devel gcc lrzsz openssh-clients

之后上传nginx安装包 解压
进入执行
./configure
然后执行
make install
会在/usr/local/nginx 这边是生成的路径
复制conf里面的nginx.conf到外面
启动nginx命令
./nginx -s stop
./nginx -s quit
./nginx -s reload
然后指明配置文件
./nginx -c   xxxxxpath
测试nginx配置文件是否有误
./nginx -t


./nginx -c /app/nginx/conf/nginx.conf

nacos集群搭建参考网址
https://blog.csdn.net/molihuakai_118/article/details/108315719?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522161577856016780271545098%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=161577856016780271545098&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-2-108315719.first_rank_v2_pc_rank_v29&utm_term=springcloud+nacos%E6%90%AD%E5%BB%BA%E6%9C%8D%E5%8A%A1%E9%9B%86%E7%BE%A4

下载nacos的linux版本解压  1.1.4和1.4完全是2个版本，搭建集群方式是不一样的。
启动是执行startup.sh

创建数据库并且设置编码
CREATE DATABASE IF NOT EXISTS nacos_config DEFAULT CHARSET utf8 COLLATE utf8_general_ci;
修改application.properties
#将下面的配置的信息设置你的真实ip
spring.datasource.platform=mysql
db.num=1
db.url.0=jdbc:mysql://192.168.232.104:3306/nacos_config?characterEncoding=utf8&connectTimeout=1000&socketTimeout=3000&autoReconnect=true&useUnicode=true&useSSL=false&serverTimezone=UTC
db.user=root
db.password=root
db.password=root

查看防火墙状态：firewall-cmd --state

启动防火墙：systemctl start firewalld.service

关闭防火墙：systemctl stop firewalld.service

重启：systemctl restart firewalld.service

开机启用：systemctl enable firewalld.service

禁止开机启动：systemctl disable firewalld.service





第1种方式  在3个机器上分别安装nacos
备份下面这个文件
cp cluster.conf.example cluster.conf

vim cluster.conf

设置端口
192.168.16.101:8848
192.168.16.102:8848
192.168.16.103:8848




之后分别启动bin路径下的startup.sh
sh startup.sh  之后集群就可以看到效果了
但是，每次访问都要根据id:端口访问 所以设置nginx来访问

nginx配置文件在外面



下载自带的jdk
#检查是否存在jdk
rpm -qa|grep java
#卸载
rpm -e --nodeps

vim /etc/profile
#set java enviroment
export JAVA_HOME=/app/java/jdk/jdk1.8.0_281
export JRE_HOME=/app/java/jdk/jdk1.8.0_281/jre
export CLASSPATH=.:$JAVA_HOME/lib$:JRE_HOME/lib:$CLASSPATH
export PATH=$JAVA_HOME/bin:$JRE_HOME/bin/$JAVA_HOME:$PATH

使配置文件生效
[root@localhost java]# source /etc/profile
测试

java-version

启动nginx指明配置文件
[root@localhost sbin]# ./nginx -c /app/nginx/nginx-1.8.0/conf/nginx.conf


---------------------------------------
开始学习Sentinel熔断和限流
官网
https://github.com/alibaba/Sentinel/wiki/%E4%BB%8B%E7%BB%8D

Hystrix缺点
1.需要我们程序员自己手工搭建监控平台。
2.没有一套web界面可以给我们进行更加细粒度得配置
流控，速率控制，服务熔断，服务降级。


阿波罗配置管理中心，只是听说

Sentinel ，分布式系统的流量防卫兵
1.独立一个组件，可以独立出来
2.直接界面化的细粒度统一配置。（约定>配置>编码）
都可以写在代码里面，但是我们本次还是大规模的学习使用配置和注解的方式，尽量少写代码。
3.Sentinel 承接了阿里巴巴近 10 年的双十一大促流量的核心场景，例如秒杀

怎么安装和配置
下载地址
https://github.com/alibaba/Sentinel/releases
可以用1.7或者1.8
下载jar包
直接可以运行在java环境，需要注意，端口8080不要被占用
java -jar sentinel-dashboard-1.7.1.jar
之后就可以访问http://localhost:8080
账号密码都是sentinel

之后怎么整合到项目中
新建8401项目
cloudalibaba-sentinel-service8401
搭建好之后访问接口进行测试
之后再访问就可以看到效果了
http://localhost:8080/#/dashboard/metric/cloudalibaba-sentinel-service

流控规则
资源名:唯一名称，默认请求路径

针对来源：Sentinel可以针对调用者进行限流，填写微服务名，默认default(不区分来源)

阈值类型/单机阈值
    QPS（每秒钟的请求数量）：当调用该API的QPS达到阈值的时候，进行限流
    线程数：当调用该API的线程数达到阈值的时候，进行限流。
    (2个东西不一样，OPS：所有的请求都在外面，在外面进行拦截，一秒一次 其他的拦截。 而线程数是在门里面，
    进行拦截，只能有一个线程)
是否集群：不需要集群

流控模式：
    直接：API达到限流条件时，直接限流
    关联：当关联的资源达到阈值时，就限流自己（当与A关联的资源B达到阈值后，就限流自己，简单来说，B惹事，A挂了，这种场景，比如说支付服务和下单服务，支付挂了就无法下单）。
    链路：只记录指定链路上的流量（指定资源从入口资源进来的流量，如果达到阈值，就进行限流）【api级别的针对来源】

流控效果：
    快速失败：直接失败，抛出异常
    Warm Up:根据codeFactor（冷加载因子，默认3）的值，从阈值/CodeFactor，经过预热时长，才达到设置的QPS阈值（默认 就是阈值/3 开始）
    排队等待：匀速排队，让请求以匀速的速度通过，阈值类型必须设置QPS，否则无效

---------------------------------------------------
使用流控，点击簇点链路，按钮来添加
默认选择，针对来源选择default，阈值类型，qps，单机阈值为1 确定就可以了
表示1秒钟内查询一次就是OK，若超过次数1，就直接--快速失败，报默认错误。
之后快速访问的时候就会直接出错。


但是这样调用的是默认的报错信息，技术方面OK，是否应该有我们自己的后续处理
兜底方法，分为系统默认和客户自定义 2种

从HystrixCommand 到@SentinelResource

使用postman进行并发压力测试。流控模式选择关联，对A进行编辑，
关联资源为/testB ,当频繁调用testB， B满足了条件，A就不能使用。但是B不会挂，只有A调用的时候会报错。


使用链路方式
资源名为公共接口,为message
入口资源为/info1
1.yml中新增配置
<dependency>
            <groupId>com.alibaba.csp</groupId>
            <artifactId>sentinel-web-servlet</artifactId>
</dependency>

2.yml中修改 spring.application.cloud.sentinel.web-context-unify=false
spring.application.cloud.sentinel.filter.enabled=false
3.新增配置类FilterContextConfig

流控效果Warm Up效果
默认codeFactor 为3 ，即请求QPS从(threshold/3)开始,经多少预热时长才逐渐升至设定的QPS阈值。
案列，阈值为10  ，预热为5秒

系统初始化的阈值为10/3 约等于3，即阈值刚开始为3，然后过了5秒后阈值慢慢升高恢复到10

实际案例：秒杀系统在开启的瞬间，会有很多流量进来，很有可能把系统打死，
预热方式就是为了保护系统，可慢慢的把流量放进来，慢慢的把阈值长到设置的阈值。


匀速排队，让请求以均匀的速度通过，阈值类型必须设成QPS，否则无效。漏桶算法
设置含义：/testA 每秒1次请求，超过的话就排队等待，等待的超时时间为20000毫秒


下面来学习降级规则

RT（平均响应时间，秒级）
    平均响应时间， 超出阈值 且 在时间窗口内通过的请求>=5 2个条件同时满足后触发降级。
    窗口期过后关闭断路器
    RT最大4900（更大的需要通过-Dcsp.sentinel.statistic.max.rt=XXXX 才能生效）

异常比例（秒级）
QPS>=5 且异常比例（妙级统计）超过阈值时，触发降级，时间窗口结束后，关闭降级

异常数（分钟级）
异常数（分钟统计）超过阈值时，触发降级，时间窗口结束后，关闭降级

Sentinel 熔断降级会调用链路中某个资源出现不稳定状态时（例如调用超时或异常比例升高，）对这个资源的调用进行限制，
让请求快速失败，避免影响到其它的资源而导致的级联错误。

当资源呗降级后，在接下来的降级时间窗口之内，对该资源的调用都自动熔断（默认行为是抛出DegradeException）。

Sentinel的断路器是没有半开状态的（1.7没有，1.8之后就有了）。
半开的状态系统自动去检测是否请求有异常，
没有异常就关闭断路器恢复使用，
有异常则继续打开断路器不可用，具体可以参考Hystrix

----------------------------------------
实战慢调用比例SLOW_REQUEST_RATIO（1.7里面不可用，所以我这边版本使用1.8.1的最新版本）
选择慢调用比例作为阈值，需要设置允许的慢调用RT（也就是最大响应时间，默认为毫秒ms，可以设置200毫秒），
请求的响应时间大于该值则统计为慢调用。

当单位统计时长（statIntervalMs）内请求数目 大于设置的最小请求数目， 并且慢调用的
比例大于阈值，则接下来的熔断时长内请求会自动被熔断。

熔断时长：在这段时间内发生熔断，拒绝所有请求。

最小请求数：即允许通过的最小请求数，在该数量内不发生熔断

熔断有三种状态，分别为OPEN，HALF_OPEN,CLOSED
OPEN:表示熔断开启，拒绝所有请求。
HALF_OPEN:探测恢复状态，如果接下来的一个请求顺利通过则结束熔断，否则继续熔断。
CLOSED:表示熔断关闭，顺利通过

经过熔断时长后熔断器会进入探测恢复状态（HALF-OPEN 状态），若接下来的一个请求响应时间小于设置的
慢调用RT 则结束熔断，若大于设置的慢调用RT则再次被熔断。


最大RT  为1
比例阈值(0-1之间)为0.5
熔断时长:10s
最小请求数:设置为5，每秒5个
统计时长:1000 ms
当资源的响应时间超过最大RT（以ms为单位，最大RT即最大响应时间）之后，资源进入准降级状态。
如果接下来1s内持续进入5个请求（最小请求数），它们的RT都持续超过这个阈值，那么在接下来的熔断时长之内，就会对这个方法进行服务降级。

满足2个条件会触发熔断，
1：单位统计时长内请求数大于设置的最小值，
2.慢请求达到设置的比例。

异常比例
比例阈值 在0,1之间， 百分比，比如设置为0.5  也就是百分之50
最小请求数设置为5.
同时满足2个条件才可以出发降级
1.每秒请求数超过5个   >=5
2. 异常比例超过50%
设置一个接口， int age=10/0;
每秒访问10个请求 调用就会触发。


配置解释说明----
单独访问一次，必然来一次报错一次 (int age=10/0)，调一次错一次。
异常比例:当资源的每秒请求亮>=5,并且每秒异常总数占通过了的比例超过阈值(DegradeRule的count)之后，
资源进入降级状态，即在接下的时间窗口（DegradeRule的timeWindow，以s为单位）之内，
对这个方法的调用都会自动的返回，异常比率的阈值范围是[0.0,1.0]，代表0%--100%

开启jmeer后，直接高并发发送请求，多次调用达到我们的配置条件了，
断路器开启（保险丝跳闸），微服务不可用，不再报错error 而是服务降级了。

异常数----------------------------
通过计算发生异常的请求数于设置阈值对比的一种策略

当资源近（统计时长内）的异常数目超过阈值（也就是你设置的异常数）之后会进行服务降级，
统计时间窗口是可以设置的，之前的是不可以设置的，

---------------------------------------------------------------
热点规则（非常实用和常用）
何为热点？
热点即经常访问的数据，很多时候我们希望统计某个热点数据中访问频次最高的Top K数据，并对其访问进行限制。
比如：商品id作为参数， 统计一段时间内最常购买的商品id进行限制。
     用户id作为参数，针对一段时间内频繁访问的用户id进行限制。
热点参数限流会统计传入参数中的热点参数，并根据配置的限流阈值与模式，
对包含热点参数的资源调用进行限流。
热点参数限流可以看做是一种特殊的流量控制，仅对包含热点参数的资源调用生效。

怎么用 兜底方法为deal_testHotkey
@SentinelResource(value = "testHotkey",blockHandler = "deal_testHotkey")
sentinel中心必须对资源testHotkey设置热点。
参数索引为0,  下标从0开始。
单机阈值为1，
统计窗口时长为1秒

配置说明：每秒的请求超过1次QPS，就会触发降级，会调用兜底方法。

参数例外项
普通配置，超过1秒钟一个后，达到阈值会立刻进行限流
我们期望p1参数当它是某个特殊值时，它的限流值和平时不一样。

特列：当p1的值等于5的时候，阈值就可以为200 所以出现了参数例外项
参数类型为参数的具体类型，
参数值为5，阈值为200,。
一定要点击添加，否则不会保存。

之后进行测试，当p1的值为5，阈值就可以到200才会限流，否则不会限流，
不为5的时候会进行普通限流
注意jmeter调用例外项测试的时候要加参数

注意：
@SentinelResource
处理的是Sentinel控制台配的违规情况，有blockHandler方法配置的兜底处理

RuntimeException
int age=10/0  ; 这个是java程序运行时报的异常，@SentinelResource是不会管的。

总结：@SentinelResource主管配置出错，运行出错该走异常走异常。

-------------------------------------------------------------------------------------------
系统规则
Sentinel 系统自适应限流从整体维度对应用入口流量进行控制，
结合应用的 Load、CPU 使用率、总体平均 RT、入口 QPS 和并发线程数等几个维度的监控指标，  （重点）
通过自适应的流控策略，让系统的入口流量和系统的负载达到一个平衡，让系统尽可能跑在最大吞吐量的同时保证系统整体的稳定性。

Load 自适应（仅对 Linux/Unix-like 机器生效，window不行）：系统的 load1 作为启发指标，进行自适应系统保护。
当系统 load1 超过设定的启发值，且系统当前的并发线程数超过估算的系统容量时才会触发系统保护（BBR 阶段）。系统容量由系统的 maxQps * minRt 估算得出。设定参考值一般是 CPU cores * 2.5。

CPU usage（1.5.0+ 版本）：当系统 CPU 使用率超过阈值即触发系统保护（取值范围 0.0-1.0），比较灵敏。  百分比，和入口QPS差不多

平均 RT：当单台机器上所有入口流量的平均 RT 达到阈值即触发系统保护，单位是毫秒。 平均处理时间

并发线程数：当单台机器上所有入口流量的并发线程数达到阈值即触发系统保护。

入口 QPS：当单台机器上所有入口流量的 QPS 达到阈值即触发系统保护。 所有入口。

测试1：入口QPS（CPU），设置为1，1秒钟超过1次QPS，就会降级， 不管访问哪个接口 都是一样的， 但是这个限流方式不常用于生产，容易产生问题。

-----------------------------------------------------------------------------
自定义限流，解决代码膨胀问题
使用一个方法一个熔断会造成以下问题
1.系统默认的，没有体现我们自己的业务要求。
2.自定义处理方法和业务代码耦合在一块，不直观。
3.每个业务方法都添加一个兜底的，会代码膨胀。
4.全局统一出发的方法没有体现，

怎么做全局异常，设置一个类。CustomerBlockHandler，里面的方法必须是static方法，否则会报错，
2个方法都是降级方法，其他的普通接口指向这个就可以了，比如说
@SentinelResource(value = "customerBlockHandler",blockHandlerClass = CustomerBlockHandler.class,blockHandler = "handlderException")


@SentinelResource不支持private

实际工作中用
sentinel整合ribbon+openFeign+fallback

cloudalibaba-consumer-nacos-order84是Ribbon整合
新建2个服务提供者
cloudalibaba-provider-payment9004
cloudalibaba-provider-payment9005
之后创建服务调用者84
cloudalibaba-consumer-nacos-order84
进行限流和降级说明
fallback是执行程序异常走的兜底方法
blockHandler 负责的是sentinel配置违规
如果两个都配置的情况，java程序有异常并且QPS比较高的情况下，
则限流降级而抛出BlockException时 只会进入blockHandler处理逻辑。

cloudalibaba-consumer-nacos-order85是openFeign整合，注意依赖的maven包
yml中激活sentinel对Feign的支持
feign.sentinel.enabled=true
启动类中的修改 说明是和Feign整合
@EnableFeignClients

具体使用看项目cloudalibaba-consumer-nacos-order85
-------------------------------------------------------------------------------------
sentinel配置持久化
将限流配置规则持久化进Nacos保存，只要刷新8401某个rest地址，sentinel控制台
的流控规则就能看到，只要Nacos里面的配置不删除，针对8401上sentinel上的流控规则就持续有效
具体配置
pom中新增
<!--后续做持久化会用到 -->
        <dependency>
            <groupId>com.alibaba.csp</groupId>
            <artifactId>sentinel-datasource-nacos</artifactId>
        </dependency>

yml中新增
sentinel下面的配置
datasource:
        ds1:
          nacos:
            server-addr: localhost:8848
            dataId: ${spring.application.name}
            groupId: DEFAULT_GROUP
            data-type: json
            rule-type: flow

在nacos注册中心新增配置
Data ID: cloudalibaba-sentinel-service
GROUP: DEFAULT_GROUP
配置格式为json
内容为以下
[{
    "resource": "byUrl",
    "limitApp": "default",
    "grade": 1,
    "count": 1,
    "strategy": 0,
    "controlBehavior": 0,
    "clusterMode": false

}]

配置说明
resource是资源名称
limitApp 来源应用
grade： 阈值类型  0表示线程数  1表示QPS
count：单机阈值
strategy：流控模式，0表示直接，1关联，2链路
controlBehavior 流控效果：0快速失败，1表示Warm Up。2表示排队等待
clusterMode 是否集群


-------------------------------------------------------------------------------------
分布式事务Seata

单体应用被拆分成微服务应用，原来的三个模块被拆分为3个独立的应用，
分别使用三个独立的数据源。
业务操作需要调用三个方位来完成，此时每个服务内部的数据一致性由本地事务来保证
但是全局的数据一致性问题没法保证。

一句话：一次业务操作需要跨多个系统进行进行远程调用，就会产生分布式问题。
Transaction ID XID  全局唯一的事务ID
3组件概念
Transaction Coordinator（TC）事务协调器，维护全局事务的运行状态，负责协调并驱动全局事务的提交或回滚
Transaction Manager(TM) 控制全局事务的边界，负责开启一个全局事务，并最终发起全局提交或者全局回滚的决议。
Resource Manager(RM) 控制分支事务，负责分支注册，状态汇报，并接收事务协调器的指令，驱动分支（本地）
事务的提交和回滚

处理过程
1.TM向TC申请开启一个全局事务，全局事务创建成功并生成一个全局唯一的XID；
2.XID在微服务调用链路的上下文中传播
3.RM向TC注册分支事务，将其纳入XID对应全局事务的管辖。
4.TM向TC发起针对XID的全局提交或回滚决议。
5、TC调度XID下管辖的全部分支事务完成提交或回滚请求。

seata下载地址
https://github.com/seata/seata/tags
下载好之后解压
1.修改conf里面的file.conf文件
修改store的mode等于db ，存储方式为db
接下来修改db的实际连接和密码
2.之后创建需要数据库的表
在README-zh.md里面找到
https://github.com/seata/seata/tree/develop/script/server
选择mysql，创建数据库seata
3.修改registry.conf文件
设置type为nacos

修改好就可以启动seata-server.bat，之后可以看到nacos注册中心的服务seata-server

创建3个服务，订单服务，库存服务，账户服务
当用户下单时，会再订单服务中创建一个订单，然后通过远程调用库存服务来扣减下单商品的库存，
再通过远程调用账户服务来扣减用户账户里面的余额，
最后在订单服务中修改订单状态为已完成。
seata_order:订单的数据库          t_order
seata_storage:存储库存的数据库      t_storage
seata_account:账户信息的数据库      t_account
创建sql在src目录下
3个库中都需要建立各自的回滚日志表undo_log
地址为
https://github.com/seata/seata/blob/develop/script/client/at/db/mysql.sql

业务需求    下订单----减少库存-------扣余额------改订单状态
Order-Module
Storage-Module
Account-Module

创建seata-order-service2001,创建seata-storage-service2002,
seata-account-service2003

注意resources里面的file.conf中service的vgroupMapping
@Param 和@RequestParam 区别，  @Param是用于dao层 和mapper.xml文件接收参数
@requestParam是controller接收外面url发送的参数

seata部署集群----》
有2种方式 配置信息是完全一样的，
1.3个服务器分别部署然后用nginx调用
2. 一个服务器部署3个节点，启动seata-server.bat -p 18091 -n 1
说明-p是端口 -n 1 表示第一个节点

seata原理(AT模式)：
TC：也就是seata服务器，seataServer
TM：@GlobalTransactional 事务的发起方，
RM：一个数据库就是一个RM，事务的参与方

执行流程：
TM开启分布式事务（TM向TC注册全局事务记录）
按业务场景，编排数据库，服务等事务内资源，（RM向TC汇报资源准备状态）；
TM结束分布式事务，事务一阶段结束（TM通知TC提交/回滚分布式事务）
TC 汇总事务信息，决定分布式事务是提交还是回滚。
TC通知所有RM提交/回滚，事务二阶段结束。


在一阶段，Seata会拦截 “业务SQL”
1   解析SQL语句，找到“业务SQL”，要更新的业务数据，在业务数据被更新前，将其保存为"before image",
2.  执行“业务SQL”，更新业务数据，在业务数据更新之后，
3. 其保存成“after image”，最后生成行锁。
以上操作全部在一个数据库事务内完成，这样保证了一阶段操作的原子性。

二阶段如果是顺利提交的话，
因为“业务”在一阶段已经提交至数据库，所以Seata框架只需要将一阶段的快照数据和行锁删掉，完成数据清理即可。

二阶段回滚：
二阶段如果是回滚的话，Seata就需要回滚一阶段已经执行的 “业务SQL”，还原业务数据。
回滚的方式是用“before image"，还原业务数据；  但在还原前要首先校验脏写，对比”数据库当前业务数据“和”after image“，
如果两份数据完全一致 就说明 没有脏写，可以还原业务数据，如果不一致就说明有脏写，
出现脏写就需要转人工处理。

校验增写 “after image” VS 数据库业务数据
还原数据： “before image”  -》逆向SQL，---》数据还原
删除中间数据  删除 “before image”，“删除after image“，删除行锁


seata有AT模式，TCC模式，SAGA模式， TC-TM-RM属于AT模式（无侵入自动补偿的事务）。
AT 模式是一种对业务无任何侵入的分布式事务解决方案。

相对于 AT 模式，TCC 模式对业务代码有一定的侵入性，但是 TCC 模式无 AT 模式的全局行锁，TCC 性能会比 AT 模式高很多。
Sage
1.一阶段提交本地数据库事务，无锁，高性能
2.补偿服务即正向服务的 “反向”，高吞吐
3.参与者可异步执行，高吞吐
AT 模式是无侵入的分布式事务解决方案，适用于不希望对业务进行改造的场景，几乎0学习成本。
TCC 模式是高性能分布式事务解决方案，适用于核心系统等对性能有很高要求的场景。
Saga 模式是长事务解决方案，适用于业务流程长且需要保证事务最终一致性的业务系统，Saga 模式一阶段就会提交本地事务，无锁，长流程情况下可以保证性能，
多用于渠道层、集成层业务系统。事务参与者可能是其它公司的服务或者是遗留系统的服务，无法进行改造和提供 TCC 要求的接口，也可以使用 Saga 模式。


XA模式是分布式强一致性的解决方案，但性能低而使用较少。


TCC模式，性能比AT模型要强，高性能，
TCC 模式对业务代码有一定的侵入性，但是 TCC 模式无 AT 模式的全局行锁，TCC 性能会比 AT 模式高很多。

TCC-3001。TCC-3002，TCC-3003 项目都是TCC

Sage适用于业务流程长/多，
参与者包含其他公司或遗留系统服务，无法提供 TCC 模式要求的三个接口
典型业务系统：如金融网络（与外部金融机构对接）、互联网微贷、渠道整合、分布式架构服务集成等业务系统

有三种异常：
空补偿：原服务未执行，补偿服务执行了
悬挂：补偿服务比原服务先执行、
幂等：原服务与补偿服务都需要保证幂等性

优点
一阶段提交本地数据库事务，无锁，高薪能
补偿服务即正向服务的 “反向”，高吞吐
参与者可异步执行，高吞吐
暂时没有例子。



https://blog.csdn.net/qq_39554452/article/details/109988012?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522161787015116780261979682%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&request_id=161787015116780261979682&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_v2~rank_v29-6-109988012.first_rank_v2_pc_rank_v29&utm_term=seata+tcc%E6%A1%88%E4%BE%8B


seata中分布式事务
@GlobalTransactional(name = "fsp-create-order",rollbackFor = Exception.class)


开始看142









